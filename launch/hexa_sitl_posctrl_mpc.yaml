# Directory to the learned model parameters
learned_model_params: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/hexa_sitl/my_models/hexa_sitl_linear_nonoise_sde.pkl

input_constr:
  # pwm_1, pwm_2, pwm_3, pwm_4
  input_id: [0, 1, 2, 3, 4, 5]
  input_bound: [[1.0e-4, 1.], [1.0e-4,1.], [1.0e-4, 1.], [1.0e-4, 1.], [1.0e-4, 1.], [1.0e-4, 1.]]

# Enforce the control inputs bound during learning value function
enforce_ubound: True


# Cost for tracking trajectory
# cost_params:
#   uref: [0.42, 0.42, 0.42, 0.42, 0.42, 0.42]
#   uerr: 0.01 # m1, m2, m3, m4 1.0
#   perr: [5., 5., 15.] # x, y, z, vx, vy, vz
#   verr: [0.1, 0.1, 0.1]
#   qerr: [0.1, 0.1, 5] # qx, qy, qz
#   werr: [0.1, 0.1, 0.5] # wx, wy, wz
#   # res_sig: [0, 0.1]
#   res_mult: 1.0

# Cost for tracking position setpoint
state_constr:
  # It represents the indexes of the constrained state variables
  state_id: [3, 4, 5, 10, 11, 12]
  # Penalty term for the slack variables
  state_penalty: [10.0, 10.0, 20.0, 1.0, 1.0, 1.0] # 0.1
  # slack scaling and constraints
  # Set of min max values for the slack id above
  slack_scaling: [3.0, 3.0, 3.0, 3.0, 3.0, 3.0]
  state_bound: [[-0.5, 0.5], [-0.5, 0.5], [-0.4, 0.4], [-0.8, 0.8], [-0.8, 0.8], [-0.7, 0.7]] #.inf, -.inf
  # Enforce bound constraints via proximal ?
  # Doing so augment the number of variables of the problem by the number of
  # slack constraints
  slack_proximal: True
  constr_pen: 0.1

cost_params:
  uref: [0.42, 0.42, 0.42, 0.42, 0.42, 0.42]
  uerr: 0.1 # m1, m2, m3, m4 1.0
  perr: [10., 10., 30.] # x, y, z
  verr: [2.0, 2.0, 2.0] # vx, vy, vz
  qerr: [0.1, 0.1, 10] # qx, qy, qz
  werr: [0.1, 0.1, 0.1] # wx, wy, wz
  # res_sig: [0, 0.1]
  res_mult: 0.1
  u_slew_coeff: 1.0

horizon: 20
num_short_dt: 20
short_step_dt: 0.05
# DANGEROUS: To use a time step different higher than the one used during training
long_step_dt: 0.05
discount: 1.0

# Number of particles when sampling the SDE
num_particles: 1


# Optimizer parameters
apg_mpc:
  # The intial step size in case no linsearh arguments are provided
  stepsize: 1.

  # The maximum number of gradient updates
  max_iter: 200
  max_no_improvement_iter: 200

  # The adaptive coefficient to scale the momentum. nill values mean
  # that it is not used and rather beta_k = k /(k+3) is used as classical acceleration momentum
  # This value should be between 0 and 1
  moment_scale: null

  # The initial momentum.
  beta_init: 0.25

  # # The stoppng criteria of the algorithm based on gradient norm
  atol: 1.0e-8 # The minimum cost difference or 'zero' cost value
  rtol: 1.0e-6

  linesearch:
    max_stepsize: 0.1 # The maximum admissible step size
    coef: 0.01 # The agressiveness coefficient. The smaller the larger step size in the optimization
    decrease_factor: 0.7 # The decrease factor when performing the armijo linesearch
    increase_factor: 1.3 # The increase factor at each new gradient descent iteration
    # # The reset strategy at each iteration
    # # "conservative": re-use previous stepsize, producing a non increasing sequence of stepsizes. Slow convergence.
    # # "increase": attempt to re-use previous stepsize multiplied by increase_factor. Cheap and efficient heuristic.
    reset_option: increase # or conservative
    maxls: 4 # Maximum number of iterations during the line search
# Directory to the learned model parameters
learned_model_params: '/home/franckdjeumou/catkin_ws/src/mpc4px4/mpc4px4/modelling/iris/my_models/sitl_iris_full_sde_params.pkl'

# # # Bounds constraints setting for the states
# # # Slack variables are created in lieu of the states --> Only for bounds constraints
# # # These are the hidden states for which bound constraints are imposed
# state_constr:
#   # It represents the indexes of the constrained state variables
#   # state_id: [1, 3, 4]
#   # # Penalty term for the slack variables
#   # state_penalty: [1., 0.1, 10.] # 0.1
#   # # slack constraints
#   # # Set of min max values for the slack id above
#   # state_bound: [[7, 12], [12.5,15], [-0.7, 0.7]] #.inf, -.inf
#   state_id: [1, 4]
#   # Penalty term for the slack variables
#   state_penalty: [1, 100] # 0.1
#   # slack constraints
#   # Set of min max values for the slack id above
#   state_bound: [[8.0, 10.0], [-0.7, 0.7]] #.inf, -.inf
#   # Enforce bound constraints via proximal ?
#   # Doing so augment the number of variables of the problem by the number of
#   # slack constraints
#   slack_proximal: True
# input constraints

input_constr:
  # pwm_1, pwm_2, pwm_3, pwm_4
  input_id: [0, 1, 2, 3]
  input_bound: [[0., 1.], [0.,1.], [0., 1.], [0., 1.]]


# Enforce the control inputs bound during learning value function
enforce_ubound: True


# Control parameters -> Weighting in the cost function
cost_params:
  uref: [0.7, 0.7, 0.7, 0.7]
  uerr: [0.1, 0.1, 0.1, 0.1] # m1, m2, m3, m4 1.0
  perr: [1000., 1000., 1000., 10., 10., 10.] # x, y, z, vx, vy, vz
  qerr: [1.,1., 10.] # qx, qy, qz
  werr: [10., 10., 0.1] # wx, wy, wz


# Number of particles when sampling the SDE
num_particles: 1

horizon: 40
num_short_dt: 40
short_step_dt: 0.01
# DANGEROUS: To use a time step different higher than the one used during training
long_step_dt: 0.01
discount: 1.0


# Optimizer parameters
apg_mpc:
  # The intial step size in case no linsearh arguments are provided
  stepsize: 1.

  # The maximum number of gradient updates
  max_iter: 100
  max_no_improvement_iter: 100

  # The adaptive coefficient to scale the momentum. nill values mean
  # that it is not used and rather beta_k = k /(k+3) is used as classical acceleration momentum
  # This value should be between 0 and 1
  moment_scale: null

  # The initial momentum.
  beta_init: 0.25

  # # The stoppng criteria of the algorithm based on gradient norm
  atol: 1.0e-8 # The minimum cost difference or 'zero' cost value
  rtol: 1.0e-5

  linesearch:
    max_stepsize: 10.0 # The maximum admissible step size
    coef: 0.01 # The agressiveness coefficient. The smaller the larger step size in the optimization
    decrease_factor: 0.7 # The decrease factor when performing the armijo linesearch
    increase_factor: 1.3 # The increase factor at each new gradient descent iteration
    # # The reset strategy at each iteration
    # # "conservative": re-use previous stepsize, producing a non increasing sequence of stepsizes. Slow convergence.
    # # "increase": attempt to re-use previous stepsize multiplied by increase_factor. Cheap and efficient heuristic.
    reset_option: increase # or conservative
    maxls: 4 # Maximum number of iterations during the line search

# Directory to the learned model parameters
# learned_model_params: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/hexa_ahg/my_models/hexa_ahg_linear_nonoise_v3_sde.pkl
learned_model_params: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/hexa_ahg/my_models/hexa_ahg_v4_sde.pkl
# trajectory_path: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/iris_sitl/my_data/test_traj_gen.csv
trajectory_path: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/iris_sitl/my_data/vcircle_traj_gen.csv


input_constr:
  # pwm_1, pwm_2, pwm_3, pwm_4
  input_id: [0, 1, 2, 3, 4, 5]
  input_bound: [[1.0e-4, 1.], [1.0e-4,1.], [1.0e-4, 1.], [1.0e-4, 1.], [1.0e-4, 1.], [1.0e-4, 1.]]

# Enforce the control inputs bound during learning value function
enforce_ubound: True

cost_params:
  uref: [0.33, 0.33, 0.33, 0.33, 0.33, 0.33]
  uerr: 1.0 # m1, m2, m3, m4 1.0
  perr: [100., 100., 200.] # x, y, z
  verr: [5.0, 5.0, 10.0] # vx, vy, vz
  qerr: [1, 1, 100] # qx, qy, qz
  werr: [1.0, 1.0, 1.0] # wx, wy, wz
  # res_sig: [0, 0.1]
  res_mult: 0.01
  u_slew_coeff: 1.0


horizon: 20
num_short_dt: 20
short_step_dt: 0.05
# DANGEROUS: To use a time step different higher than the one used during training
long_step_dt: 0.05
discount: 1.0

# Number of particles when sampling the SDE
num_particles: 1

# Optimizer parameters
apg_mpc:
  # The intial step size in case no linsearh arguments are provided
  stepsize: 1.

  # The maximum number of gradient updates
  max_iter: 200
  max_no_improvement_iter: 200

  # The adaptive coefficient to scale the momentum. nill values mean
  # that it is not used and rather beta_k = k /(k+3) is used as classical acceleration momentum
  # This value should be between 0 and 1
  moment_scale: null

  # The initial momentum.
  beta_init: 0.25

  # # The stoppng criteria of the algorithm based on gradient norm
  atol: 1.0e-8 # The minimum cost difference or 'zero' cost value
  rtol: 1.0e-6

  linesearch:
    init_stepsize: 0.01
    max_stepsize: 1.0 # The maximum admissible step size
    coef: 0.01 # The agressiveness coefficient. The smaller the larger step size in the optimization
    decrease_factor: 0.7 # The decrease factor when performing the armijo linesearch
    increase_factor: 1.3 # The increase factor at each new gradient descent iteration
    # # The reset strategy at each iteration
    # # "conservative": re-use previous stepsize, producing a non increasing sequence of stepsizes. Slow convergence.
    # # "increase": attempt to re-use previous stepsize multiplied by increase_factor. Cheap and efficient heuristic.
    reset_option: increase # or conservative
    maxls: 4 # Maximum number of iterations during the line search
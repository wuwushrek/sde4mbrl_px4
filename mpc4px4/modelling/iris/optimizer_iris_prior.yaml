# An example of config file used to train the geometric parameters of the quadcopter
# along with the thurst and moment response to motors inputs

# Path the the log data
logs_dir:
  - /home/franckdjeumou/Documents/log_flights/iris_sitl_1.ulg
  - /home/franckdjeumou/Documents/log_flights/iris_sitl_2.ulg
  - /home/franckdjeumou/Documents/log_flights/iris_sitl_0.ulg
  # More trajectories if available

# Testing trajectory
test_trajectory: /home/franckdjeumou/Documents/log_flights/iris_sitl_test.ulg

# Seed for random number genertaion
seed: 0

# Directory of prior model file. This is where the outputs are going to be saved
vehicle_dir: ~/catkin_ws/src/mpc4px4/mpc4px4/modelling/iris

# Prior model parameter path
prior_file: iris_prior.yaml

# Enforce that the trajectory log is filtered every time
# When false, the trajectory is only filtered the first time and save on the disk
force_filtering: False

# Cutoff frequency for filtering
# The details on how to find them can be seen in filter_data_analysis.ipynb
cutoff_freqs:
  x: 2
  y: 2
  z: 2
  vx: 4
  vy: 4
  vz: 4
  qw: 6
  qx: 6
  qy: 6
  qz: 6
  wx: 6
  wy: 6
  wz: 6

# Loss function parameters
# Parameters to weight objective in the loss function
loss:
  pen_xdot: 10.0                # Weight coefficient on the vector field
  pen_motor_constr: 10.0        # Weight coefficient on the maximum speed from the motors
  pen_init_dev: 1.0             # Weight coefficient on the deviation from the prior values on the model
  pen_params: 0.0               # Weight on all the params of the problem


# Optimizer parameters
optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  - name: exponential_decay
    scheduler: True
    params:
      init_value: -0.01 # Initial learning rate (Negative value for minimization)
      transition_steps: 80000 # Basically the maximum number of gradient steps
      decay_rate: 0.01 # Typically (end_value / init_value) if end_value is expected at end
  # - name: adaptive_grad_clip
  #   params:
  #     clipping: 0.01

# Training parameters
training:
  train_batch_per_traj: 4096 # The mini batch size for the training dataset
  test_batch_size: 1024 # The mini batch used for evaluating the training trajectory

  nepochs: 20000 # The number of epochs (full pass over the training dataset)
  
  patience: 5000 # The number of epochs after which to stop the learningif no improvement in solution

  test_freq: 100 # Number of gradient steps after which to print the loss and all
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  display_freq: 2000 # Number of gradient steps after which we lot the current solution

  key_to_show: null

  show_plot: True # Show plots of the forces evolution
  size_to_plot: 150000 # Plot all dataset
  display_losses: True # Display the losses as the training evolves

# An example of config file used to train the static parameters of the quadcopter

# Path the the log data
logs_dir:
  - /home/franckdjeumou/Documents/log_flights/log_sitl_2022-10-22-15-53-52.ulg
  - /home/franckdjeumou/Documents/log_flights/log_sitl_2022-10-25-13-57-58.ulg
  # More trajectories if available

test_trajectory: /home/franckdjeumou/Documents/log_flights/log_6_2022-10-25-14-03-16.ulg

# Seed for random number genertaion
seed: 0

# Prior model parameter path
prior_model_path: /home/franckdjeumou/catkin_ws/src/mpc4px4/launch/iris_model_prior.yaml

# Cutoff frequency for filtering
force_filtering: False

# The details on how to find them can be seen in filter_data_analysis.ipynb
cutoff_freqs:
  x: 2
  y: 2
  z: 2
  vx: 4
  vy: 4
  vz: 4
  qw: 6
  qx: 6
  qy: 6
  qz: 6
  wx: 6
  wy: 6
  wz: 6

# Loss function parameters
# Parameters to weight objective in the loss function
loss:
  pen_xdot: 10.0               # Weight coefficient on the vector field
  pen_motor_constr: 10.0      # Weight coefficient on the maximum speed from the motors
  pen_init_dev: 100.0         # Weight coefficient on the deviation from the prior values on the model
  pen_params: 0.0


# Optimizer parameters
optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  - name: exponential_decay
    scheduler: True
    params:
      init_value: -0.01 # Initial learning rate (Negative value for minimization)
      transition_steps: 50000 # Basically the maximum number of gradient steps
      decay_rate: 0.01 # Typically (end_value / init_value) if end_value is expected at end
  # - name: adaptive_grad_clip
  #   params:
  #     clipping: 0.01

# # Optimizer parameters
# optimizer:
#   name: ArmijoSGD
#   params:
#     max_stepsize: 1.0
#     aggressiveness: 0.8
  
# optimizer:
#   name: PolyakSGD
#   params:
#     max_stepsize: 1.0

# Training parameters
training:
  train_batch_per_traj: 4096 # The mini batch size for the training dataset
  test_batch_size: 4096 # The mini batch used for evaluating the training trajectory

  nepochs: 15000 # The number of epochs (full pass over the training dataset)
  
  patience: 5000 # The number of epochs after which to stop the learningif no improvement in solution

  test_freq: 100 # Number of gradient steps after which to print the loss and all
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  display_freq: 2000 # Number of gradient steps after which we lot the current solution

  key_to_show: null

  show_plot: True # Show plots of the forces evolution
  size_to_plot: 150000 # Plot all dataset
  display_losses: True # Display the losses as the training evolves
